<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深度学习 | Vincent^s Blog</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/wwx13/assets/css/0.styles.2328f29d.css" as="style"><link rel="preload" href="/wwx13/assets/js/app.5f87f3d6.js" as="script"><link rel="preload" href="/wwx13/assets/js/2.c594a943.js" as="script"><link rel="preload" href="/wwx13/assets/js/19.82bf2432.js" as="script"><link rel="prefetch" href="/wwx13/assets/js/10.ec1d9a87.js"><link rel="prefetch" href="/wwx13/assets/js/100.6dd3924c.js"><link rel="prefetch" href="/wwx13/assets/js/101.0797e51c.js"><link rel="prefetch" href="/wwx13/assets/js/102.2627e912.js"><link rel="prefetch" href="/wwx13/assets/js/103.0ff02bff.js"><link rel="prefetch" href="/wwx13/assets/js/104.a2fe8a86.js"><link rel="prefetch" href="/wwx13/assets/js/105.cd1d12b8.js"><link rel="prefetch" href="/wwx13/assets/js/106.cdfed742.js"><link rel="prefetch" href="/wwx13/assets/js/107.1a7b40af.js"><link rel="prefetch" href="/wwx13/assets/js/108.ce5ef96d.js"><link rel="prefetch" href="/wwx13/assets/js/109.71c20038.js"><link rel="prefetch" href="/wwx13/assets/js/11.4a62537e.js"><link rel="prefetch" href="/wwx13/assets/js/110.87289e2b.js"><link rel="prefetch" href="/wwx13/assets/js/111.60814e48.js"><link rel="prefetch" href="/wwx13/assets/js/112.01a952c9.js"><link rel="prefetch" href="/wwx13/assets/js/113.f2c142df.js"><link rel="prefetch" href="/wwx13/assets/js/114.3ff63502.js"><link rel="prefetch" href="/wwx13/assets/js/115.6a843b6e.js"><link rel="prefetch" href="/wwx13/assets/js/116.d98301e0.js"><link rel="prefetch" href="/wwx13/assets/js/117.4611b4b1.js"><link rel="prefetch" href="/wwx13/assets/js/118.2dc979af.js"><link rel="prefetch" href="/wwx13/assets/js/119.6f4f4de8.js"><link rel="prefetch" href="/wwx13/assets/js/12.cc9ad852.js"><link rel="prefetch" href="/wwx13/assets/js/13.a972c8ea.js"><link rel="prefetch" href="/wwx13/assets/js/14.48818404.js"><link rel="prefetch" href="/wwx13/assets/js/15.30b75f93.js"><link rel="prefetch" href="/wwx13/assets/js/16.9c778972.js"><link rel="prefetch" href="/wwx13/assets/js/17.49c7d556.js"><link rel="prefetch" href="/wwx13/assets/js/18.ff2fe358.js"><link rel="prefetch" href="/wwx13/assets/js/20.8139b403.js"><link rel="prefetch" href="/wwx13/assets/js/21.3f1359d8.js"><link rel="prefetch" href="/wwx13/assets/js/22.ce867378.js"><link rel="prefetch" href="/wwx13/assets/js/23.e772744d.js"><link rel="prefetch" href="/wwx13/assets/js/24.02e90b25.js"><link rel="prefetch" href="/wwx13/assets/js/25.27019d8f.js"><link rel="prefetch" href="/wwx13/assets/js/26.551e853d.js"><link rel="prefetch" href="/wwx13/assets/js/27.e77c7496.js"><link rel="prefetch" href="/wwx13/assets/js/28.db4dbfd7.js"><link rel="prefetch" href="/wwx13/assets/js/29.32157689.js"><link rel="prefetch" href="/wwx13/assets/js/3.ad6b2af0.js"><link rel="prefetch" href="/wwx13/assets/js/30.33820301.js"><link rel="prefetch" href="/wwx13/assets/js/31.d0171516.js"><link rel="prefetch" href="/wwx13/assets/js/32.8e64af5a.js"><link rel="prefetch" href="/wwx13/assets/js/33.54c58a16.js"><link rel="prefetch" href="/wwx13/assets/js/34.2f57efb9.js"><link rel="prefetch" href="/wwx13/assets/js/35.1a534956.js"><link rel="prefetch" href="/wwx13/assets/js/36.b132dac3.js"><link rel="prefetch" href="/wwx13/assets/js/37.a6ad15e2.js"><link rel="prefetch" href="/wwx13/assets/js/38.c6de11d8.js"><link rel="prefetch" href="/wwx13/assets/js/39.27aa33e3.js"><link rel="prefetch" href="/wwx13/assets/js/4.26bed799.js"><link rel="prefetch" href="/wwx13/assets/js/40.3ba36d01.js"><link rel="prefetch" href="/wwx13/assets/js/41.0ef19e64.js"><link rel="prefetch" href="/wwx13/assets/js/42.23902c35.js"><link rel="prefetch" href="/wwx13/assets/js/43.4e9e68bb.js"><link rel="prefetch" href="/wwx13/assets/js/44.743d355c.js"><link rel="prefetch" href="/wwx13/assets/js/45.ab1194dc.js"><link rel="prefetch" href="/wwx13/assets/js/46.86794f77.js"><link rel="prefetch" href="/wwx13/assets/js/47.2cc24490.js"><link rel="prefetch" href="/wwx13/assets/js/48.46698598.js"><link rel="prefetch" href="/wwx13/assets/js/49.605a30fb.js"><link rel="prefetch" href="/wwx13/assets/js/5.6650c683.js"><link rel="prefetch" href="/wwx13/assets/js/50.229d0151.js"><link rel="prefetch" href="/wwx13/assets/js/51.3e3e561b.js"><link rel="prefetch" href="/wwx13/assets/js/52.396d6a37.js"><link rel="prefetch" href="/wwx13/assets/js/53.468afda0.js"><link rel="prefetch" href="/wwx13/assets/js/54.1f8ee031.js"><link rel="prefetch" href="/wwx13/assets/js/55.da39011a.js"><link rel="prefetch" href="/wwx13/assets/js/56.6d79feba.js"><link rel="prefetch" href="/wwx13/assets/js/57.e31cbecd.js"><link rel="prefetch" href="/wwx13/assets/js/58.7f6e43f0.js"><link rel="prefetch" href="/wwx13/assets/js/59.b7085302.js"><link rel="prefetch" href="/wwx13/assets/js/6.876d9396.js"><link rel="prefetch" href="/wwx13/assets/js/60.8843d6d1.js"><link rel="prefetch" href="/wwx13/assets/js/61.8c0267ea.js"><link rel="prefetch" href="/wwx13/assets/js/62.33a32dc2.js"><link rel="prefetch" href="/wwx13/assets/js/63.acf0182c.js"><link rel="prefetch" href="/wwx13/assets/js/64.051bc4f0.js"><link rel="prefetch" href="/wwx13/assets/js/65.8a6e73b5.js"><link rel="prefetch" href="/wwx13/assets/js/66.149a4556.js"><link rel="prefetch" href="/wwx13/assets/js/67.5d38c8d3.js"><link rel="prefetch" href="/wwx13/assets/js/68.88e04af7.js"><link rel="prefetch" href="/wwx13/assets/js/69.a8e378a6.js"><link rel="prefetch" href="/wwx13/assets/js/7.88bb0522.js"><link rel="prefetch" href="/wwx13/assets/js/70.7f8a80b5.js"><link rel="prefetch" href="/wwx13/assets/js/71.18f7da61.js"><link rel="prefetch" href="/wwx13/assets/js/72.6fee7d94.js"><link rel="prefetch" href="/wwx13/assets/js/73.3a264c19.js"><link rel="prefetch" href="/wwx13/assets/js/74.6e1fe6a4.js"><link rel="prefetch" href="/wwx13/assets/js/75.62afc61b.js"><link rel="prefetch" href="/wwx13/assets/js/76.48359efd.js"><link rel="prefetch" href="/wwx13/assets/js/77.e391f3fa.js"><link rel="prefetch" href="/wwx13/assets/js/78.91e51175.js"><link rel="prefetch" href="/wwx13/assets/js/79.40fe0d03.js"><link rel="prefetch" href="/wwx13/assets/js/8.e09cee93.js"><link rel="prefetch" href="/wwx13/assets/js/80.0039dff0.js"><link rel="prefetch" href="/wwx13/assets/js/81.e9d529c0.js"><link rel="prefetch" href="/wwx13/assets/js/82.1094019e.js"><link rel="prefetch" href="/wwx13/assets/js/83.8245350a.js"><link rel="prefetch" href="/wwx13/assets/js/84.3e9495ce.js"><link rel="prefetch" href="/wwx13/assets/js/85.5c3bb05f.js"><link rel="prefetch" href="/wwx13/assets/js/86.43c6e0c7.js"><link rel="prefetch" href="/wwx13/assets/js/87.ddf86b1d.js"><link rel="prefetch" href="/wwx13/assets/js/88.ddca3c8e.js"><link rel="prefetch" href="/wwx13/assets/js/89.18d878bb.js"><link rel="prefetch" href="/wwx13/assets/js/9.4de0dc10.js"><link rel="prefetch" href="/wwx13/assets/js/90.a1e8c5ee.js"><link rel="prefetch" href="/wwx13/assets/js/91.de442e64.js"><link rel="prefetch" href="/wwx13/assets/js/92.49c0824b.js"><link rel="prefetch" href="/wwx13/assets/js/93.6896ca17.js"><link rel="prefetch" href="/wwx13/assets/js/94.d7cc7b7a.js"><link rel="prefetch" href="/wwx13/assets/js/95.bf25d412.js"><link rel="prefetch" href="/wwx13/assets/js/96.147dbe17.js"><link rel="prefetch" href="/wwx13/assets/js/97.e0da221c.js"><link rel="prefetch" href="/wwx13/assets/js/98.c86a38ea.js"><link rel="prefetch" href="/wwx13/assets/js/99.72f4a787.js">
    <link rel="stylesheet" href="/wwx13/assets/css/0.styles.2328f29d.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/wwx13/" class="home-link router-link-active"><!----> <span class="site-name">Vincent^s Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/wwx13/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/wwx13/structure_and_algo/" class="nav-link">
  数据结构与算法
</a></div><div class="nav-item"><a href="/wwx13/nlp/" class="nav-link router-link-active">
  NLP
</a></div><div class="nav-item"><a href="/wwx13/big_data/" class="nav-link">
  big data
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数学、统计" class="dropdown-title"><span class="title">数学、统计</span> <span class="arrow down"></span></button> <button type="button" aria-label="数学、统计" class="mobile-dropdown-title"><span class="title">数学、统计</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/wwx13/math/matrix/" class="nav-link">
  线性代数
</a></li><li class="dropdown-item"><!----> <a href="/wwx13/math/derivate/" class="nav-link">
  微积分
</a></li><li class="dropdown-item"><!----> <a href="/wwx13/math/optimization/" class="nav-link">
  优化理论
</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/wwx13/" class="nav-link">
  主页
</a></div><div class="nav-item"><a href="/wwx13/structure_and_algo/" class="nav-link">
  数据结构与算法
</a></div><div class="nav-item"><a href="/wwx13/nlp/" class="nav-link router-link-active">
  NLP
</a></div><div class="nav-item"><a href="/wwx13/big_data/" class="nav-link">
  big data
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数学、统计" class="dropdown-title"><span class="title">数学、统计</span> <span class="arrow down"></span></button> <button type="button" aria-label="数学、统计" class="mobile-dropdown-title"><span class="title">数学、统计</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/wwx13/math/matrix/" class="nav-link">
  线性代数
</a></li><li class="dropdown-item"><!----> <a href="/wwx13/math/derivate/" class="nav-link">
  微积分
</a></li><li class="dropdown-item"><!----> <a href="/wwx13/math/optimization/" class="nav-link">
  优化理论
</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/wwx13/" aria-current="page" class="sidebar-link">Vincet的bolg</a></li><li><a href="/wwx13/nlp/" aria-current="page" class="sidebar-link">自然语言处理</a></li><li><a href="/wwx13/math/matrix/" class="sidebar-link">线性代数</a></li><li><a href="/wwx13/math/derivate/" class="sidebar-link">微积分</a></li><li><a href="/wwx13/structure_and_algo/" class="sidebar-link">数据结构与算法</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="attention"><a href="#attention" class="header-anchor">#</a> Attention</h1> <h2 id="seq2seq-attention"><a href="#seq2seq-attention" class="header-anchor">#</a> seq2seq attention</h2> <p>编码器-解码器结构里，背景变量是固定为编码器的一个关于各时间步隐状态的函数输出。
引入attention，将使得各个解码器时间步使用不同的背景变量：
对于当前时刻，背景变量为加权的编码器各时间步隐状态。权重计算方式为：
接收当前解码器隐状态和编码器某时刻隐状态为输入的函数输出 再以所有时候函数输出为分母做softmax。
<img src="/wwx13/assets/img/att1.fd7f528b.png" alt="att1"> <img src="/wwx13/assets/img/att2.33ee724f.png" alt="att2"> <img src="/wwx13/assets/img/seq2seq_attention.909e903d.png" alt="Ane"></p> <h3 id="更新隐状态-gru"><a href="#更新隐状态-gru" class="header-anchor">#</a> 更新隐状态(gru)</h3> <p>对候选隐状态计算修改——引入当前时候attention计算出来的背景向量：</p> <p>重置门:激活函数内部加上了背景向量*权重</p> <p>更新门也是</p> <p>候选隐状态计算tanh内部加上了候选隐状态*权重</p> <p><img src="/wwx13/assets/img/att3.3ec8609c.png" alt="att3"></p> <div class="custom-block tip"><p class="custom-block-title">TIP</p> <p>实际上，我们只需要把解码器当前时刻的特征和经过attention计算的背景向量在特征维度拼接即可使用原生gru计算。</p></div> <h3 id="简单版实现"><a href="#简单版实现" class="header-anchor">#</a> 简单版实现</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span>
                 attention_size<span class="token punctuation">,</span> drop_prob<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> attention_model<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>num_hiddens<span class="token punctuation">,</span> attention_size<span class="token punctuation">)</span>
        <span class="token comment"># GRU的输入包含attention输出的c和实际输入, 所以尺寸是 num_hiddens+embed_size</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>num_hiddens <span class="token operator">+</span> embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> 
                          num_layers<span class="token punctuation">,</span> dropout<span class="token operator">=</span>drop_prob<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> cur_input<span class="token punctuation">,</span> state<span class="token punctuation">,</span> enc_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        cur_input shape: (batch, )
        state shape: (num_layers, batch, num_hiddens)
        &quot;&quot;&quot;</span>
        <span class="token comment"># 使用注意力机制计算背景向量</span>
        c <span class="token operator">=</span> attention_forward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention<span class="token punctuation">,</span> enc_states<span class="token punctuation">,</span> state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 将嵌入后的输入和背景向量在特征维连结, (批量大小, num_hiddens+embed_size)</span>
        input_and_c <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>cur_input<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 为输入和背景向量的连结增加时间步维，时间步个数为1</span>
        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>input_and_c<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 移除时间步维，输出形状为(批量大小, 输出词典大小)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state

    <span class="token keyword">def</span> <span class="token function">begin_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态</span>
        <span class="token keyword">return</span> enc_state

</code></pre></div><h3 id="复杂版实现"><a href="#复杂版实现" class="header-anchor">#</a> 复杂版实现</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> keras <span class="token keyword">import</span> backend <span class="token keyword">as</span> K
<span class="token keyword">from</span> keras <span class="token keyword">import</span> regularizers<span class="token punctuation">,</span> constraints<span class="token punctuation">,</span> initializers<span class="token punctuation">,</span> activations
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent <span class="token keyword">import</span> Recurrent<span class="token punctuation">,</span> _time_distributed_dense
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>engine <span class="token keyword">import</span> InputSpec
 
tfPrint <span class="token operator">=</span> <span class="token keyword">lambda</span> d<span class="token punctuation">,</span> T<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>Print<span class="token punctuation">(</span>input_<span class="token operator">=</span>T<span class="token punctuation">,</span> data<span class="token operator">=</span><span class="token punctuation">[</span>T<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> message<span class="token operator">=</span>d<span class="token punctuation">)</span>
 
<span class="token keyword">class</span> <span class="token class-name">AttentionDecoder</span><span class="token punctuation">(</span>Recurrent<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> units<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span>
                 activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span>
                 return_probabilities<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                 name<span class="token operator">=</span><span class="token string">'AttentionDecoder'</span><span class="token punctuation">,</span>
                 kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span>
                 recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span>
                 bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span>
                 kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Implements an AttentionDecoder that takes in a sequence encoded by an
        encoder and outputs the decoded states
        :param units: dimension of the hidden state and the attention matrices
        :param output_dim: the number of labels in the output space
 
        references:
            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.
            &quot;Neural machine translation by jointly learning to align and translate.&quot;
            arXiv preprint arXiv:1409.0473 (2014).
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>units <span class="token operator">=</span> units
        self<span class="token punctuation">.</span>output_dim <span class="token operator">=</span> output_dim
        self<span class="token punctuation">.</span>return_probabilities <span class="token operator">=</span> return_probabilities
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> activations<span class="token punctuation">.</span>get<span class="token punctuation">(</span>activation<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>kernel_initializer <span class="token operator">=</span> initializers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>kernel_initializer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>recurrent_initializer <span class="token operator">=</span> initializers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>recurrent_initializer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias_initializer <span class="token operator">=</span> initializers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bias_initializer<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>kernel_regularizer <span class="token operator">=</span> regularizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>kernel_regularizer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>recurrent_regularizer <span class="token operator">=</span> regularizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>kernel_regularizer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias_regularizer <span class="token operator">=</span> regularizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bias_regularizer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activity_regularizer <span class="token operator">=</span> regularizers<span class="token punctuation">.</span>get<span class="token punctuation">(</span>activity_regularizer<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>kernel_constraint <span class="token operator">=</span> constraints<span class="token punctuation">.</span>get<span class="token punctuation">(</span>kernel_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>recurrent_constraint <span class="token operator">=</span> constraints<span class="token punctuation">.</span>get<span class="token punctuation">(</span>kernel_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias_constraint <span class="token operator">=</span> constraints<span class="token punctuation">.</span>get<span class="token punctuation">(</span>bias_constraint<span class="token punctuation">)</span>
 
        <span class="token builtin">super</span><span class="token punctuation">(</span>AttentionDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>name <span class="token operator">=</span> name
        self<span class="token punctuation">.</span>return_sequences <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># must return sequences</span>
 
    <span class="token keyword">def</span> <span class="token function">build</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473
          for model details that correspond to the matrices here.
        &quot;&quot;&quot;</span>
 
        self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>timesteps<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_dim <span class="token operator">=</span> input_shape
 
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>stateful<span class="token punctuation">:</span>
            <span class="token builtin">super</span><span class="token punctuation">(</span>AttentionDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>states <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># y, s</span>
 
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Matrices for creating the context vector
        &quot;&quot;&quot;</span>
 
        self<span class="token punctuation">.</span>V_a <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'V_a'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_a <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_a'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U_a <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'U_a'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>kernel_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_a <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'b_a'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_constraint<span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Matrices for the r (reset) gate
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>C_r <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'C_r'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U_r <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'U_r'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_r <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_r'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_r <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'b_r'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_constraint<span class="token punctuation">)</span>
 
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Matrices for the z (update) gate
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>C_z <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'C_z'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U_z <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'U_z'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_z <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_z'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_z <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'b_z'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_constraint<span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Matrices for the proposal
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>C_p <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'C_p'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U_p <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'U_p'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_p <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_p'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_p <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'b_p'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_constraint<span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Matrices for making the final prediction vector
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>C_o <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'C_o'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U_o <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'U_o'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_o'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_o <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'b_o'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>bias_constraint<span class="token punctuation">)</span>
 
        <span class="token comment"># For creating the initial state:</span>
        self<span class="token punctuation">.</span>W_s <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   name<span class="token operator">=</span><span class="token string">'W_s'</span><span class="token punctuation">,</span>
                                   initializer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_initializer<span class="token punctuation">,</span>
                                   regularizer<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_regularizer<span class="token punctuation">,</span>
                                   constraint<span class="token operator">=</span>self<span class="token punctuation">.</span>recurrent_constraint<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>input_spec <span class="token operator">=</span> <span class="token punctuation">[</span>
            InputSpec<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>timesteps<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>built <span class="token operator">=</span> <span class="token boolean">True</span>
 
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># store the whole sequence so we can &quot;attend&quot; to it at each timestep</span>
        self<span class="token punctuation">.</span>x_seq <span class="token operator">=</span> x
 
        <span class="token comment"># apply the a dense layer over the time dimension of the sequence</span>
        <span class="token comment"># do it here because it doesn't depend on any previous steps</span>
        <span class="token comment"># thefore we can save computation time:</span>
        self<span class="token punctuation">.</span>_uxpb <span class="token operator">=</span> _time_distributed_dense<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x_seq<span class="token punctuation">,</span> self<span class="token punctuation">.</span>U_a<span class="token punctuation">,</span> b<span class="token operator">=</span>self<span class="token punctuation">.</span>b_a<span class="token punctuation">,</span>
                                             input_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span>
                                             timesteps<span class="token operator">=</span>self<span class="token punctuation">.</span>timesteps<span class="token punctuation">,</span>
                                             output_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>units<span class="token punctuation">)</span>
 
        <span class="token keyword">return</span> <span class="token builtin">super</span><span class="token punctuation">(</span>AttentionDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>call<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">get_initial_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># apply the matrix on the first time step to get the initial s0.</span>
        s0 <span class="token operator">=</span> activations<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_s<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
        <span class="token comment"># from keras.layers.recurrent to initialize a vector of (batchsize,</span>
        <span class="token comment"># output_dim)</span>
        y0 <span class="token operator">=</span> K<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  <span class="token comment"># (samples, timesteps, input_dims)</span>
        y0 <span class="token operator">=</span> K<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y0<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (samples, )</span>
        y0 <span class="token operator">=</span> K<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>y0<span class="token punctuation">)</span>  <span class="token comment"># (samples, 1)</span>
        y0 <span class="token operator">=</span> K<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>y0<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
 
        <span class="token keyword">return</span> <span class="token punctuation">[</span>y0<span class="token punctuation">,</span> s0<span class="token punctuation">]</span>
 
    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> states<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
        ytm<span class="token punctuation">,</span> stm <span class="token operator">=</span> states
 
        <span class="token comment"># repeat the hidden state to the length of the sequence</span>
        _stm <span class="token operator">=</span> K<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>stm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>timesteps<span class="token punctuation">)</span>
 
        <span class="token comment"># now multiplty the weight matrix with the repeated hidden state</span>
        _Wxstm <span class="token operator">=</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>_stm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_a<span class="token punctuation">)</span>
 
        <span class="token comment"># calculate the attention probabilities</span>
        <span class="token comment"># this relates how much other timesteps contributed to this one.</span>
        et <span class="token operator">=</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>activations<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>_Wxstm <span class="token operator">+</span> self<span class="token punctuation">.</span>_uxpb<span class="token punctuation">)</span><span class="token punctuation">,</span>
                   K<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>self<span class="token punctuation">.</span>V_a<span class="token punctuation">)</span><span class="token punctuation">)</span>
        at <span class="token operator">=</span> K<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>et<span class="token punctuation">)</span>
        at_sum <span class="token operator">=</span> K<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>at<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        at_sum_repeated <span class="token operator">=</span> K<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>at_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>timesteps<span class="token punctuation">)</span>
        at <span class="token operator">/=</span> at_sum_repeated  <span class="token comment"># vector of size (batchsize, timesteps, 1)</span>
 
        <span class="token comment"># calculate the context vector</span>
        context <span class="token operator">=</span> K<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>K<span class="token punctuation">.</span>batch_dot<span class="token punctuation">(</span>at<span class="token punctuation">,</span> self<span class="token punctuation">.</span>x_seq<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># ~~~&gt; calculate new hidden state</span>
        <span class="token comment"># first calculate the &quot;r&quot; gate:</span>
 
        rt <span class="token operator">=</span> activations<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>
            K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>ytm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_r<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>stm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>U_r<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>context<span class="token punctuation">,</span> self<span class="token punctuation">.</span>C_r<span class="token punctuation">)</span>
            <span class="token operator">+</span> self<span class="token punctuation">.</span>b_r<span class="token punctuation">)</span>
 
        <span class="token comment"># now calculate the &quot;z&quot; gate</span>
        zt <span class="token operator">=</span> activations<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>
            K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>ytm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_z<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>stm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>U_z<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>context<span class="token punctuation">,</span> self<span class="token punctuation">.</span>C_z<span class="token punctuation">)</span>
            <span class="token operator">+</span> self<span class="token punctuation">.</span>b_z<span class="token punctuation">)</span>
 
        <span class="token comment"># calculate the proposal hidden state:</span>
        s_tp <span class="token operator">=</span> activations<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
            K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>ytm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_p<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span><span class="token punctuation">(</span>rt <span class="token operator">*</span> stm<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>U_p<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>context<span class="token punctuation">,</span> self<span class="token punctuation">.</span>C_p<span class="token punctuation">)</span>
            <span class="token operator">+</span> self<span class="token punctuation">.</span>b_p<span class="token punctuation">)</span>
 
        <span class="token comment"># new hidden state:</span>
        st <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>zt<span class="token punctuation">)</span><span class="token operator">*</span>stm <span class="token operator">+</span> zt <span class="token operator">*</span> s_tp
 
        yt <span class="token operator">=</span> activations<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>
            K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>ytm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>stm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>U_o<span class="token punctuation">)</span>
            <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>context<span class="token punctuation">,</span> self<span class="token punctuation">.</span>C_o<span class="token punctuation">)</span>
            <span class="token operator">+</span> self<span class="token punctuation">.</span>b_o<span class="token punctuation">)</span>
 
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>return_probabilities<span class="token punctuation">:</span>
            <span class="token keyword">return</span> at<span class="token punctuation">,</span> <span class="token punctuation">[</span>yt<span class="token punctuation">,</span> st<span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> yt<span class="token punctuation">,</span> <span class="token punctuation">[</span>yt<span class="token punctuation">,</span> st<span class="token punctuation">]</span>

</code></pre></div><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter10_natural-language-processing/10.12_machine-translation" target="_blank" rel="noopener noreferrer">dive into dl<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://xiehuateng.github.io/2018/07/28/tf-nmt-seq2seq/" target="_blank" rel="noopener noreferrer">ref<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://zhuanlan.zhihu.com/p/47929039" target="_blank" rel="noopener noreferrer">ref1<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/" target="_blank" rel="noopener noreferrer">ref2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/wwx13/assets/js/app.5f87f3d6.js" defer></script><script src="/wwx13/assets/js/2.c594a943.js" defer></script><script src="/wwx13/assets/js/19.82bf2432.js" defer></script>
  </body>
</html>
