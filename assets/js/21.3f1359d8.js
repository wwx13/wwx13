(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{410:function(t,s,a){t.exports=a.p+"assets/img/word_piece.0485d55f.png"},411:function(t,s,a){t.exports=a.p+"assets/img/unilm.ec828d4f.png"},412:function(t,s,a){t.exports=a.p+"assets/img/unilm2.54aa0431.png"},413:function(t,s,a){t.exports=a.p+"assets/img/subword.b81894eb.png"},604:function(t,s,a){"use strict";a.r(s);var n=a(44),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"字节对编码-bpe-bytepair-encoding"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#字节对编码-bpe-bytepair-encoding"}},[t._v("#")]),t._v(" 字节对编码（BPE, BytePair Encoding)")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("1. BPE共现概率最高的对进行合并。\n2. wordpiece 重复信息最高（相关）的对合并。\n3. unilm 贡献观察到样本的概率最低的子词进行移除,viterbi确定分词序列。\n")])])]),n("h2",{attrs:{id:"词表构建"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#词表构建"}},[t._v("#")]),t._v(" 词表构建")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("词表构建是BPE算法的核心，其是「根据训练语料」来构建BPE算法的词表。算法的整体步骤如下所示：\n\n1.准备模型的训练语料\n2.确定「期望的词表大小」\n3.将训练语料中的所有单词拆分为字符序列，利用这些字符序列构建初始的词表\n4.统计训练语料中每一个连续字节对出现的频率，「选择出现频率最高的字节对合并成新的subword，并更新词表」\n5.重复第4步，直到词表大小达到我们设定的期望或者剩下的字节对出现频率最高为1\n")])])]),n("h2",{attrs:{id:"语料编码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#语料编码"}},[t._v("#")]),t._v(" 语料编码")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("词表构建好后，我们需要给训练语料中的单词进行编码。编码方式如下：\n\n我们首先「将词表中所有的子词按照长度从大到小进行排序」\n对于每一个给定的单词，我们遍历排序好的词表，寻找词表中的子词是否是该单词的子字符串。如果正好「匹配」，则输出当前子词，并对单词剩下的字符串继续匹配\n如果遍历完词表，单词中仍然有子字符串没有被匹配，那我们将其替换为一个特殊的子词，比如<unk>。\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" re"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" collections\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_stats")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pairs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" collections"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("defaultdict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" freq "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" vocab"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        symbols "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("symbols"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            pairs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("symbols"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("symbols"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" freq\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" pairs\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("merge_vocab")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pair"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v_in"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    v_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    bigram "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("escape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pair"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    p "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'(?<!\\S)'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" bigram "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'(?!\\S)'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" word "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" v_in"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        w_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" p"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sub"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pair"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        v_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("w_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v_in"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("word"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" v_out\n\nvocab "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'l o w </w>'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'l o w e r </w>'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n e w e s t </w>'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w i d e s t </w>'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\nnum_merges "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_merges"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pairs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_stats"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" pairs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n    best "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pairs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pairs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    vocab "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" merge_vocab"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print output")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('e', 's')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('es', 't')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('est', '</w>')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('l', 'o')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('lo', 'w')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('n', 'e')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('ne', 'w')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('new', 'est</w>')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('low', '</w>')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('w', 'i')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('wi', 'd')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('wid', 'est</w>')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('low', 'e')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('lowe', 'r')")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ('lower', '</w>')")]),t._v("\n")])])]),n("h1",{attrs:{id:"wordpiece"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#wordpiece"}},[t._v("#")]),t._v(" WordPiece")]),t._v(" "),n("p",[t._v("WordPiece，从名字好理解，它是一种子词粒度的tokenize算法subword tokenization algorithm，很多著名的Transformers模型，比如BERT/DistilBERT/Electra都使用了它。")]),t._v(" "),n("p",[t._v("它的原理非常接近BPE，不同之处在于它做合并时，并不是直接找最高频的组合，而是找能够最大化训练数据似然的merge。即它每次合并的两个字符串A和B，应该具有最大的值。合并AB之后，所有原来切成A+B两个tokens的就只保留AB一个token，整个训练集上最大似然变化量与成正比。\n"),n("img",{attrs:{src:a(410),alt:"wp"}})]),t._v(" "),n("h1",{attrs:{id:"unigram"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#unigram"}},[t._v("#")]),t._v(" Unigram")]),t._v(" "),n("p",[t._v("总结BPE和WordPiece，我们发现它们都是通过一定的方法，生成一个最终的词表，然后对于某个单词或句子，根据这个最终的词表，生成唯一的划分结果。")]),t._v(" "),n("p",[t._v("但是，这个划分结果一定就是最好的吗？假设通过BPE或WordPiece，生成了一张词表est"),t._v(", st"),t._v(", w, e, s, t，那么对于单词west，它有三种划分方式：w est"),t._v("，w e st"),t._v("和w e s t。根据前两个算法的编码规则，我们选择了第一个，但是也许在后续做训练任务时，其实第三个的表现更好呢？")]),t._v(" "),n("p",[t._v("因此，ULM就作为一种改进办法出现了。概括来说，它的改进思路是：")]),t._v(" "),n("p",[t._v("首先拥有一种大词表\n基于这张词表，对所有语料的所有子词划分结果做一个总体评分。然后从词表中剔除掉那些对总体评分贡献最小的子词\n重复第二步，不断丢弃贡献度小的子词，直到词表达到预设大小\n生成最终的词表。基于该词表，一个语料仍可能有多种划分，通过某种方式，选择其中最优的划分。\n基于这个改进思想，来详细看下算法过程。")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),n("ol",[n("li",[t._v("认为观察到一个句子的最大概率就是观察到所有生成方式（分词)的概率之和")]),t._v(" "),n("li",[t._v("认为词表是全局适配的，因此是句子概率连乘取log。")]),t._v(" "),n("li",[t._v("EM方法求解词表的固定概率")]),t._v(" "),n("li",[t._v("viterbi算法在生成词表中是确定可能序列，在解码时输出最可能分词分式。")])])]),t._v(" "),n("p",[n("img",{attrs:{src:a(411),alt:"s"}}),t._v(" "),n("img",{attrs:{src:a(412),alt:"s1"}})]),t._v(" "),n("p",[n("img",{attrs:{src:a(413),alt:"sub"}}),t._v(" "),n("a",{attrs:{href:"https://arxiv.org/pdf/1804.10959.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("unilm paper"),n("OutboundLink")],1),t._v(" "),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/460678461",target:"_blank",rel:"noopener noreferrer"}},[t._v("ref"),n("OutboundLink")],1),t._v(" "),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/132361501?from_voters_page=true",target:"_blank",rel:"noopener noreferrer"}},[t._v("链接原文"),n("OutboundLink")],1),t._v(" "),n("a",{attrs:{href:"https://dxzmpk.github.io/2020/04/29/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ref2"),n("OutboundLink")],1)])])}),[],!1,null,null,null);s.default=e.exports}}]);