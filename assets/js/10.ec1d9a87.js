(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{512:function(t,s,e){t.exports=e.p+"assets/img/em.30077753.png"},513:function(t,s,e){t.exports=e.p+"assets/img/em1.f2058552.png"},514:function(t,s,e){t.exports=e.p+"assets/img/em2.f3c6d16d.png"},515:function(t,s,e){t.exports=e.p+"assets/img/em3.c44e8f7a.png"},516:function(t,s,e){t.exports=e.p+"assets/img/plsa.c8736574.png"},517:function(t,s,e){t.exports=e.p+"assets/img/plsa1.20148732.png"},518:function(t,s,e){t.exports=e.p+"assets/img/plsa2.55632414.png"},519:function(t,s,e){t.exports=e.p+"assets/img/plsa3.6c1a94d6.png"},649:function(t,s,e){"use strict";e.r(s);var a=e(44),r=Object(a.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"em算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#em算法"}},[t._v("#")]),t._v(" EM算法")]),t._v(" "),a("h2",{attrs:{id:"核心思想"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#核心思想"}},[t._v("#")]),t._v(" 核心思想")]),t._v(" "),a("p",[t._v("对于标准的求函数的极小值问题，我们大部分情况下不直接求解，因为问题\n本身往往比较复杂。我们通过迭代优化。迭代优化有很多算法，对无约束优化问题，我们有随机梯度下降，牛顿法，拟牛顿法等等。\n我们说求函数最优化解，我们的迭代策略往往使得解更好好，\n什么情况下能使得我们的解变得更好呢？通用的思想是： 在当前迭代点，找一个新的函数近似目标函数曲线,目标函数是它的下界(近似函数值总是大于等于目标函数，在一定的范围内), 该函数应该具有凸性（一般而言），且能使得求解该函数的最优值也同时使得目标函数值下降。")]),t._v(" "),a("h2",{attrs:{id:"gradient-descent"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#gradient-descent"}},[t._v("#")]),t._v(" Gradient Descent")]),t._v(" "),a("p",[t._v("梯度下降实际是在当前迭代的点进行二次函数近似,在当前点和原函数一样取值，其余大于原函数值。 二次系数是常数不是hessian阵，由于近似函数是凸的， 且常数项可是\n一个大的正值，因此近似函数可以总是大于等于目前函数值， 且泰勒展开式中， 当前点的展开函数的值等于目标函数值，\n求解二次函数的最优点，会使得二次函数的最优值小展开点的展开函数值=目标函数在站开点的函数值。\n因此，随机梯度下降法可以（一般）优化参数。\n如果近似的函数的二次项常数不够大，或者问题是有约束优化问题，此时gd\n未必有效。")]),t._v(" "),a("h2",{attrs:{id:"newton-method"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#newton-method"}},[t._v("#")]),t._v(" Newton Method")]),t._v(" "),a("h2",{attrs:{id:"em"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#em"}},[t._v("#")]),t._v(" EM")]),t._v(" "),a("ol",[a("li",[t._v("确定要优化的函数和参数")]),t._v(" "),a("li",[t._v("构造合适的近似函数（关于观测变量，隐变量， 模型参数）的。\n要求是凸的，且在当前模型参数取值下，近似函数值和目标函数值是一样的。\n"),a("ul",[a("li",[t._v("近似函数的构造比如 log sum 和sum log, 后者小于前者，是前者下界。")]),t._v(" "),a("li",[t._v("当前模型参数取值点的近似函数值等于目标函数值这个约束可以用于\n求解近似函数的常量系数。例如PLSA的EM算法推导。"),a("a",{attrs:{href:"https://spaces.ac.cn/archives/4277/comment-page-3#comments",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考1"),a("OutboundLink")],1)])])]),t._v(" "),a("li",[t._v("对近似函数求模型参数的最优化解，作为更新后的模型参数。")]),t._v(" "),a("li",[t._v("直到模型参数改变很小或者达到更新代数。")])]),t._v(" "),a("p",[a("img",{attrs:{src:e(512),alt:"em"}}),t._v(" "),a("img",{attrs:{src:e(513),alt:"em1"}}),t._v(" "),a("img",{attrs:{src:e(514),alt:"em2"}}),t._v(" "),a("img",{attrs:{src:e(515),alt:"em3"}})]),t._v(" "),a("p",[a("a",{attrs:{href:"http://blog.tomtung.com/2011/10/em-algorithm/",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考2"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"plsa"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#plsa"}},[t._v("#")]),t._v(" PLSA")]),t._v(" "),a("p",[a("img",{attrs:{src:e(516),alt:"pl"}}),t._v(" "),a("img",{attrs:{src:e(517),alt:"pl2"}}),t._v(" "),a("img",{attrs:{src:e(518),alt:"pl3"}}),t._v(" "),a("img",{attrs:{src:e(519),alt:"pl4"}})]),t._v(" "),a("p",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/39116676",target:"_blank",rel:"noopener noreferrer"}},[t._v("ref"),a("OutboundLink")],1)])])}),[],!1,null,null,null);s.default=r.exports}}]);