(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{403:function(t,e,n){t.exports=n.p+"assets/img/softmax_att.ace25223.png"},404:function(t,e,n){t.exports=n.p+"assets/img/Linformer.e9465733.png"},405:function(t,e,n){t.exports=n.p+"assets/img/multi_head.420341d4.png"},601:function(t,e,n){"use strict";n.r(e);var r=n(44),a=Object(r.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"那些attention的改进工作"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#那些attention的改进工作"}},[t._v("#")]),t._v(" 那些Attention的改进工作")]),t._v(" "),r("h2",{attrs:{id:"更快的attention"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#更快的attention"}},[t._v("#")]),t._v(" 更快的attention")]),t._v(" "),r("h3",{attrs:{id:"调整计算的顺序和计算函数"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#调整计算的顺序和计算函数"}},[t._v("#")]),t._v(" 调整计算的顺序和计算函数")]),t._v(" "),r("p",[t._v("适用场景： 当序列长度很长时，模型计算attention速度很慢\n设n>>d")]),t._v(" "),r("ol",[r("li",[t._v("QKV的计算顺序： 先计算QK： n"),r("em",[t._v("n 代表每个时刻对各个时刻的相关程度，再计算")]),t._v("V 得到n"),r("em",[t._v("d.\nQ")]),t._v("的复杂度为n"),r("strong",[t._v("2， n个向量每个需要和n个向量乘。\n先计算K"),r("em",[t._v("V, 得到d")]),t._v("d 复杂度为 n"),r("em",[t._v("d 接着")]),t._v("Q, 复杂度d")]),t._v("2 ,整体O（n）。")]),t._v(" "),r("li",[t._v("但是softmax(QK/\\sqrt{d}) s这个使得不能简单先计算后面，因为标准的\nsoftmax是基于exp计算的，我们需要一直计算方式使得， 可以拆解QK的相似度。\n有三种方式。\n"),r("img",{attrs:{src:n(403),alt:"拆解"}}),t._v(" "),r("a",{attrs:{href:"%5Bhttps://arxiv.org/pdf/2006.16236.pdf%5D"}},[t._v("Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention")])])]),t._v(" "),r("h3",{attrs:{id:"稀疏attention"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#稀疏attention"}},[t._v("#")]),t._v(" 稀疏attention")]),t._v(" "),r("p",[t._v("OpenAI的Sparse Attention，通过“只保留小区域内的数值、强制让大部分注意力为零”的方式，来减少Attention的计算量。经过特殊设计之后，Attention矩阵的大部分元素都是0，因此理论上它也能节省显存占用量和计算量。后续类似工作还有《Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection》、《Longformer: The Long-Document Transformer》等\n"),r("img",{attrs:{src:n(404),alt:"Linformer"}})]),t._v(" "),r("p",[r("a",{attrs:{href:"https://kexue.fm/archives/7546/comment-page-1#comments",target:"_blank",rel:"noopener noreferrer"}},[t._v("ref"),r("OutboundLink")],1)]),t._v(" "),r("h2",{attrs:{id:"多头的表达能力改进"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#多头的表达能力改进"}},[t._v("#")]),t._v(" 多头的表达能力改进")]),t._v(" "),r("p",[t._v("适用场景：数据量充分； 文本很长")]),t._v(" "),r("ol",[r("li",[t._v("增大key_size\n由于原始的multi-head attention 的Q,K参数量实际少于预期\n表征的参数量，因此存在Q,K表征瓶颈。\n第一种方式是在得到Q,K的线性层处增大维度，而V的维度不变，这使得multi head拼接的最终维度\n依然和改进以前完全一样。\n"),r("img",{attrs:{src:n(405),alt:"多头"}}),t._v(" "),r("a",{attrs:{href:"https://arxiv.org/abs/2002.07028",target:"_blank",rel:"noopener noreferrer"}},[t._v("Low-Rank Bottleneck in Multi-head Attention Models"),r("OutboundLink")],1),r("br"),t._v("\n最后，附上我们预训练的两个增大了key_size的RoBERTa小模型，欢迎大家使用（我们称之为RoBERTa+）：")])]),t._v(" "),r("p",[t._v("https://github.com/ZhuiyiTechnology/pretrained-models\n2. 不增大key size, 我们考虑通过引入额外的参数把多个头的\nQ"),r("em",[t._v("K结果进行混合，这么做的原因有两个：1. 每个头反应在一个位置\nQ")]),t._v("K结果反应的是不同角度的相关性，因此彼此不应该独立，应该有关联。 2. 新的\n混合参数引入了更多可能，使得Q*K的表达能力增强。\n"),r("a",{attrs:{href:"https://arxiv.org/abs/2003.02436",target:"_blank",rel:"noopener noreferrer"}},[t._v("alking-Heads Attention"),r("OutboundLink")],1)]),t._v(" "),r("p",[r("a",{attrs:{href:"https://spaces.ac.cn/archives/7325/comment-page-1#comments",target:"_blank",rel:"noopener noreferrer"}},[t._v("see ref"),r("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=a.exports}}]);