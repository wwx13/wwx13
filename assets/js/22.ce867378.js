(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{461:function(e,t,n){e.exports=n.p+"assets/img/mengzi.693a9d6e.png"},462:function(e,t,n){e.exports=n.p+"assets/img/mengzi1.7b806fa0.png"},463:function(e,t,n){e.exports=n.p+"assets/img/mengzi2.e51dae6a.jpg"},464:function(e,t,n){e.exports=n.p+"assets/img/mengzi2.e8adb41e.png"},627:function(e,t,n){"use strict";n.r(t);var s=n(44),a=Object(s.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h2",{attrs:{id:"贡献"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#贡献"}},[e._v("#")]),e._v(" 贡献")]),e._v(" "),s("p",[e._v("本文的目标不是追求更大的模型规模，而是轻量级但更强大，同时对部署和工业落地更友好的模型。基于语言学信息融入和训练加速等方法，研发了 Mengzi 系列模型。由于与 BERT 保持一致的模型结构，Mengzi 模型可以快速替换现有的预训练模型。这项工作的主要贡献有三个方面：")]),e._v(" "),s("p",[e._v("1)研究了各种预训练策略来训练轻量级语言模型，表明精心设计良好的目标可以进一步显著提高模型的容量，而不需要扩大模型的大小。\n2发布了Mengzi模型，包括判别式、生成式、金融和多模态模型变体，能够胜任广泛的语言和视觉任务。这些模型中的文本编码器只包含1.03亿个参数，我们希望这能够促进学术界和工业界的相关研究。\n3)通过大量基准任务测试表明，孟子模型在一系列语言理解和生成任务上取得了很强的性能。")]),e._v(" "),s("h2",{attrs:{id:"模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模型"}},[e._v("#")]),e._v(" 模型")]),e._v(" "),s("p",[s("img",{attrs:{src:n(461),alt:"mengzi"}})]),e._v(" "),s("p",[e._v("如图所示Mengzi模型家族包括：")]),e._v(" "),s("p",[e._v("Mengzi-BERT-base\nMengzi-BERT-base-fin\nMengzi-T5-base\nMengzi-Oscar-base")]),e._v(" "),s("p",[s("img",{attrs:{src:n(462),alt:"mengzi1"}}),e._v("\n从技术角度来看，后三个可以看作是Mengzi-BERT-base的衍生，因为它们的文本编码器遵循与Mengzi-BERT-base相同的结构，并由Mengzi-BERT-base的预训练参数初始化。因此，在下面的实验部分中，文章只关注基本的仅文本编码器方面，以及相关有效的优化技术。")]),e._v(" "),s("p",[e._v("模型设置\n数据预处理：训练前的语料库来源于中文维基百科、中文新闻和爬虫语料，总数据大小为300GB。通过使用探索性的数据分析技术来清理数据，删除HTML标签、url、电子邮件、表情符号等。由于在原始语料库中有简体标记和传统的中文标记，使用OpenCC将传统标记转换为简体形式，重复的文章也会被删除。\n模型结构：RoBERTa被选做为 Mengzi预训练的骨干模型，12层transformers，hidden size为768，12个attention heads，预训练任务为MLM。\n预训练细节：(1)词汇表包含21,128个字符，看了下与Bert大小保持一致。句子长度限制在512个字符，batch size为128。(2)在训练前，每个序列中有15%的单词被随机屏蔽以进行MLM预测。(3)使用LAMB优化器的mixed-batch训练方式，它涉及两个阶段：总epoch的前9/10使用序列长度为128，总epoch的最后1/10使用序列长度为512。这两个阶段的批次规模分别为16384和32768。采用PostgreSQL对训练示例进行全局抽样，以避免两阶段训练中样本权重的不平衡。整个培训前的过程需要100万步。使用32个3090 24G，使用FP16和深度4进行训练加速（只能说土豪）")]),e._v(" "),s("h2",{attrs:{id:"result"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#result"}},[e._v("#")]),e._v(" result")]),e._v(" "),s("p",[s("img",{attrs:{src:n(463),alt:"mengzi2"}})]),e._v(" "),s("p",[e._v("微调细节\n在微调实验中，论文使用Adam作为优化器，初始学习率为{8e-6、1e-5、2e-5、3e-5}，预热率为0.1，L2权重衰减为0.01。批处理大小将在{16、24、32}中进行选择。根据任务，最大epochs在[2,5]中设置。对于文本最大长度：MRC的最大长度为384，其他任务为256。")]),e._v(" "),s("p",[e._v("PLM进阶\n文本进一步研究了预训练和微调技术，来进一步提高Menzi模型的能力。")]),e._v(" "),s("p",[e._v("预训练技术")]),e._v(" "),s("p",[e._v("Linguistic-motivated Objectives：语义信息已被证明对语言建模是有效的。受LIMIT-Bert的启发，在训练前使用了词性(POS)和命名实体(NE)序列标记任务，并结合了原始的MLM和NSP目标。原始文本中的POS和NE标签由spaCy标注。")]),e._v(" "),s("p",[e._v("Sequence Relationship Objectives：为了更好地对句子间的句子对信息进行建模，Mengzi在模型预训练中添加了句子顺序预测(SOP)任务。")]),e._v(" "),s("p",[e._v("Dynamic Gradient Correction：广泛使用的MLM会引起原始句子结构的干扰，导致语义丢失，增加了模型预测的难度，不可避免地导致训练不足和效率低下。为了缓解这一问题，本文提出了一系列的动态梯度校正技术来提高模型的性能和鲁棒性。")]),e._v(" "),s("p",[e._v("微调策略")]),e._v(" "),s("p",[e._v("Knowledge Distillation：文本训练了一个教师模型，并采用教师模型来指导学生模型的训练。详细地，分别计算了相同输入序列的上下文隐藏状态的Kullback-leibler(KL)散度。差异度量了教师和学生模型的表示之间的相似度，在微调过程中与原始下游任务目标一起最小化。")]),e._v(" "),s("p",[e._v("Transfer Learning：不同任务的迁移学习，比如利用CMNLI数据集上训练模型的参数来初始化C3等相关数据集的模型训练")]),e._v(" "),s("p",[e._v("Choice Smoothing：对于多项选择或分类任务，结合不同类型的训练目标会带来更好的表现。对于每个输入示例，我们应用交叉熵和二进制交叉熵作为损失函数，并结合双方的损失，帮助模型从不同的粒度学习特征。")]),e._v(" "),s("p",[e._v("Adversarial Training：应用了一种由平滑诱导的对抗性正则化技术：SMART，以支持当向输入注入一个小的扰动时，模型的输出没有太大的变化。")]),e._v(" "),s("p",[s("img",{attrs:{src:n(464),alt:"mz"}})])])}),[],!1,null,null,null);t.default=a.exports}}]);