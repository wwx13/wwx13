(window.webpackJsonp=window.webpackJsonp||[]).push([[93],{645:function(t,a,v){"use strict";v.r(a);var _=v(44),s=Object(_.a)({},(function(){var t=this,a=t.$createElement,v=t._self._c||a;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"bit-map"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#bit-map"}},[t._v("#")]),t._v(" Bit map")]),t._v(" "),v("p",[t._v("有海量整数M，如何给到一个整数判定在不在里面\n1 int = 4 bytes = 32 bits\n因词设定一个长度为M比特的存储单元，每个位置和整数0-M对应，0代表没有，1代表有。\n因此等价于设定长度为M//32+1的整数数组，里面是一个32bit的01存储序列。\n对M遍历，对当前整数x找到数组第x//32位的 第f个(f=x mod 32)的bit, 取1 << f 和 这个32bit做或运算，使得\n第f个bit为1，代表存在整数x。")]),t._v(" "),v("p",[t._v("查询时，对当前整数y, y//32  y mode 32 观察对应bit为1代表存在，bit为0不存在。")]),t._v(" "),v("h1",{attrs:{id:"boom-filter"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#boom-filter"}},[t._v("#")]),t._v(" Boom filter")]),t._v(" "),v("p",[t._v("对海量数据，回答当前数据在不在里面。这个方法有一定的误差率。\n思想：设有k个独立的hash函数对数据映射到数值不超过M的整数。\n初始全为0的长度为M的数组，对海量数据变量，每个数据进行k个hash，并根据结果找到对应索引的数组位，为0则赋值1。")]),t._v(" "),v("p",[t._v("查询，对于当前数据k个hash,随后查询每个下标的值，都为1则存在或者误判为存在，否则不存在。")]),t._v(" "),v("h1",{attrs:{id:"堆"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#堆"}},[t._v("#")]),t._v(" 堆")]),t._v(" "),v("p",[t._v("读入海量整数，一直累计数目，每当读入的数个数为奇数，输出中位数。")]),t._v(" "),v("p",[t._v("这题可以用两个堆完成，用一个大根堆存储中位数左边的数，用一个小根堆存储中位数右边的数。")]),t._v(" "),v("p",[t._v("因此，我们需要维护两个堆的一个性质：")]),t._v(" "),v("p",[t._v("两个堆的数字数目需要相同（为了方便，将中位数也算在左边的大根堆中，所以实际上大根堆会比小根堆的数目多一个）。")]),t._v(" "),v("p",[t._v("①每次读入一个数，判断大根堆down是否为空，若空， 就插入。或者当前的数x是否小于大根堆down的堆顶，是就插入，否则就插入小根堆up。")]),t._v(" "),v("p",[t._v("②然后判断大根堆down的数目是不是比小根堆up的数目+1还要大，如果还要大，就需要将大根堆down的堆顶元素放入小根堆up中。")]),t._v(" "),v("p",[t._v("③然后判断小根堆up的数目是不是比大根堆down的数目大，是就将小根堆up的堆顶元素插入大根堆down中。")]),t._v(" "),v("p",[t._v("然后在总数目为奇数的时候，大根堆down的堆顶元素就是当前序列的中位数。\n但是数据量很大不能直接使用双堆。")]),t._v(" "),v("h1",{attrs:{id:"多层分桶"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#多层分桶"}},[t._v("#")]),t._v(" 多层分桶")]),t._v(" "),v("p",[t._v("1).2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。\n　　有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。")]),t._v(" "),v("h1",{attrs:{id:"倒排索引"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#倒排索引"}},[t._v("#")]),t._v(" 倒排索引")]),t._v(" "),v("p",[t._v("七、倒排索引(Inverted index)")]),t._v(" "),v("p",[t._v("适用范围：搜索引擎，关键字查询")]),t._v(" "),v("p",[t._v("基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。")]),t._v(" "),v("p",[t._v('以英文为例，下面是要被索引的文本：\nT0 = "it is what it is"\nT1 = "what is it"\nT2 = "it is a banana"')]),t._v(" "),v("p",[t._v("我们就能得到下面的反向文件索引：")]),t._v(" "),v("div",{staticClass:"language- extra-class"},[v("pre",[v("code",[t._v('"a":      {2}\n"banana": {2}\n"is":     {0, 1, 2}\n"it":     {0, 1, 2}\n"what":   {0, 1}\n')])])]),v("p",[t._v('检索的条件"what","is"和"it"将对应集合的交集。')]),t._v(" "),v("p",[t._v("正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。")]),t._v(" "),v("p",[t._v("扩展：\n　　问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。")]),t._v(" "),v("h1",{attrs:{id:"外排序"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#外排序"}},[t._v("#")]),t._v(" 外排序")]),t._v(" "),v("p",[t._v("排序（External sorting）是指能够处理极大量数据的排序算法。通常来说，外排序处理的数据不能一次装入内存。（摘自百度）")]),t._v(" "),v("p",[t._v("再简单点来说。比如我们要对10亿个数进行排序。如果用int[]来存储这10亿个数的话，我们需要3*1000000000/8/1024/1024/1024≈3.7G。对于只有2G内存的电脑来说根本就跑不起来。所以这时候什么内存排序算法都玩不起来了。")]),t._v(" "),v("p",[t._v("外排序的思想是将这10亿条数据分割成N个小项（比如每个分割项放2000条数据，然后对这2000条数据进行排序[2000个int放入内存当中只需要7.8K]，排序后写入一个txt文件中）。这样就能得到N个有序的小项。再把这些小项通过运算聚合起来，得到最终结果。说白了其实就跟归并排序思路相似。")]),t._v(" "),v("h1",{attrs:{id:"一些问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一些问题"}},[t._v("#")]),t._v(" 一些问题")]),t._v(" "),v("p",[t._v("https://blog.csdn.net/fangaoxin/article/details/7305038")]),t._v(" "),v("div",{staticClass:"custom-block tip"},[v("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),v("ol",[v("li",[t._v("self-attention的计算方式，除以根号d原因")]),t._v(" "),v("li",[t._v("有3000w的一个敏感词表，有200w行文本，如何快速找到所有在200w行文本出现的敏感词")]),t._v(" "),v("li",[t._v("有2亿文本，对于一个query词例如 黄山 ； 查询在文本的出现频率。限制响应速度1s, 10G内存，10核心。")]),t._v(" "),v("li",[t._v("有10亿行词，让你构造一个函数，按照词的频率的概率每次被调用依概率采样返回一个词。")])]),t._v(" "),v("h3",{attrs:{id:""}},[v("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")])]),t._v(" "),v("p",[t._v("答案")]),t._v(" "),v("p",[t._v("1.略")]),t._v(" "),v("p",[t._v("2.对词表每个敏感词按照字符进行键分筒， 其中构建和查询按照递归进行。")]),t._v(" "),v("p",[t._v("3.暂无")]),t._v(" "),v("p",[t._v("4.构建：对词hash化，取余10000分成10000文件，内部都是保证相同词在一个文件，然后10w个平均，对这10w\n可以统计词频接着计算概率存好新的10000个文件，根据每个文件max min 概率计算全局概率上下界。\n对10000文件读取，按照max min概率等分分桶，分桶依据是概率所在等分范围，分10000份，并且每个桶的映射概率范围为min+分桶区间。\n查询：random(1) 根据结果落在10000个桶哪个上，对该桶内部的10W数据查询哪个词对应的概率范围是最接近random的采样值返回。\n主要的思想是根据分桶减少计算需要的内存和查询的数据范围。")])])])}),[],!1,null,null,null);a.default=s.exports}}]);